import logging
from typing import List, Dict, Any, Optional
from openai import OpenAI
from app.core import config
from app.services import vector_store_manager

# Configure basic logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

LLM_PROVIDER = "groq"

llm_client = None
LLM_MODEL_NAME = "" # Will be set based on provider

if LLM_PROVIDER == "groq":
    if config.GROQ_API_KEY:
        try:
            llm_client = OpenAI(
                api_key=config.GROQ_API_KEY,
                base_url="https://api.groq.com/openai/v1"
            )
            # Groq model
            LLM_MODEL_NAME = "llama3-8b-8192"
            logger.info(f"Groq client initialized. Using model: {LLM_MODEL_NAME}")
        except Exception as e:
            logger.error(f"Failed to initialize Groq client: {e}", exc_info=True)
    else:
        logger.warning("GROQ_API_KEY not found. Groq client not initialized.")
elif LLM_PROVIDER == "openai":
    if config.OPENAI_API_KEY:
        try:
            llm_client = OpenAI(api_key=config.OPENAI_API_KEY)
            LLM_MODEL_NAME = "gpt-3.5-turbo" # OpenAI model
            logger.info("OpenAI client initialized for RAG.")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client for RAG: {e}", exc_info=True)
    else:
        logger.warning("OPENAI_API_KEY not found. OpenAI client for RAG not initialized.")
else:
    logger.error(f"Unsupported LLM_PROVIDER: {LLM_PROVIDER}")

DEFAULT_TOP_K_RESULTS = 3  # Number of chunks to retrieve


def retrieve_relevant_chunks(
        query_text: str,
        collection_name: str = vector_store_manager.DEFAULT_COLLECTION_NAME,
        top_k: int = DEFAULT_TOP_K_RESULTS,
        doc_id_filter: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Retrieves relevant document chunks from the vector store based on the query.
    Embeddings for the query_text are generated by ChromaDB using the collection's EF.
    """
    collection = vector_store_manager.get_or_create_collection(collection_name)
    if not collection:
        logger.error(f"Failed to get collection '{collection_name}'. Cannot retrieve chunks.")
        return []

    try:
        # Filter if doc_id_filter is provided
        where_filter = None
        if doc_id_filter:
            where_filter = {"doc_id": doc_id_filter}
            logger.info(f"Applying filter for doc_id: {doc_id_filter}")

        results = collection.query(
            query_texts=[query_text],  # ChromaDB expects a list of query texts
            n_results=top_k,
            where=where_filter,  # Apply the filter here
            include=['documents', 'metadatas', 'distances']  # Request documents and metadatas
        )

        retrieved_chunks = []
        if results and results['documents'] and results['documents'][0]:
            for i in range(len(results['documents'][0])):
                retrieved_chunks.append({
                    "text": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i] if results['metadatas'] else {},
                    "distance": results['distances'][0][i] if results['distances'] else float('inf')
                })
            logger.info(f"Retrieved {len(retrieved_chunks)} chunks for query: '{query_text[:50]}...'")
        else:
            logger.info(f"No chunks found for query: '{query_text[:50]}...' with filter: {where_filter}")

        return retrieved_chunks
    except Exception as e:
        logger.error(f"Error querying collection '{collection_name}': {e}", exc_info=True)
        return []


def create_prompt_with_context(query_text: str, context_chunks: List[Dict[str, Any]]) -> str:
    """
    Creates a prompt for the LLM by combining the user query and retrieved context chunks.
    """
    if not context_chunks:
        # If no context found, just asking a base query to the model
        prompt = f"""You are a helpful assistant. Please answer the following question based on your general knowledge, as no specific context from documents was found.
Question: {query_text}
Answer:"""
        return prompt

    context_str = "\n\n---\n\n".join([chunk['text'] for chunk in context_chunks])

    prompt_template = f"""You are a helpful assistant. Answer the question based *only* on the following context. If the answer is not found in the context, respond with "I don't have enough information from the provided documents to answer this question." Do not use any external knowledge.

Context:
{context_str}

Question: {query_text}

Answer:"""
    logger.debug(f"Generated prompt: {prompt_template}")
    return prompt_template


def get_llm_response(prompt: str) -> Optional[str]:
    """
    Gets a response from the configured LLM (OpenAI).
    """
    if not llm_client:
        logger.error(f"{LLM_PROVIDER.capitalize()} client not initialized. Cannot get LLM response.")
        return None

    try:
        logger.info(f"Sending prompt to {LLM_PROVIDER.capitalize()} LLM (model: {LLM_MODEL_NAME}). Prompt length: {len(prompt)} chars.")
        response = llm_client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[
                {"role": "system",
                 "content": "You are a helpful assistant designed to answer questions based on provided context."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # Lower temperature for more factual, less creative answers
            max_tokens=500  # Adjust as needed
        )
        answer = response.choices[0].message.content.strip()
        logger.info(f"Received {LLM_PROVIDER.capitalize()} LLM response. Length: {len(answer)} chars.")
        return answer
    except Exception as e:
        logger.error(f"Error getting {LLM_PROVIDER.capitalize()} LLM response: {e}", exc_info=True)
        return None


def answer_query(
        user_query: str,
        collection_name: str = vector_store_manager.DEFAULT_COLLECTION_NAME,
        top_k_chunks: int = DEFAULT_TOP_K_RESULTS,
        doc_id_filter: Optional[str] = None  # New parameter to filter by doc_id
) -> Dict[str, Any]:
    """
    Orchestrates the RAG pipeline: retrieve chunks, create prompt, get LLM response.
    """
    logger.info(f"Answering query: '{user_query[:50]}...' using {LLM_PROVIDER.capitalize()} | doc_id_filter: {doc_id_filter}")

    # 1. Retrieve relevant chunks
    retrieved_chunks = retrieve_relevant_chunks(
        query_text=user_query,
        collection_name=collection_name,
        top_k=top_k_chunks,
        doc_id_filter=doc_id_filter  # Pass the filter here
    )

    if not retrieved_chunks:
        logger.warning("No relevant chunks found for the query.")
        # Return a specific message
        return {
            "query": user_query,
            "answer": "I could not find any relevant information in the documents for your query.",
            "retrieved_chunks_found": False,
            "source_chunks": []
        }

    # 2. Create prompt
    prompt = create_prompt_with_context(user_query, retrieved_chunks)

    # 3. Get LLM response
    answer = get_llm_response(prompt)

    if answer is None:
        final_answer = f"There was an error communicating with the {LLM_PROVIDER.capitalize()} language model."
    else:
        final_answer = answer

    return {
        "query": user_query,
        "answer": final_answer,
        "retrieved_chunks_found": bool(retrieved_chunks),
        "source_chunks": retrieved_chunks  # Include retrieved chunks for inspection/debugging
    }


# Example Usage (for testing this module directly)
if __name__ == "__main__":
    if not llm_client:
        print(f"{LLM_PROVIDER.capitalize()} client not initialized. Exiting RAG pipeline test.")
    else:
        print(f"\n--- RAG Pipeline Test (using {LLM_PROVIDER.capitalize()} with model {LLM_MODEL_NAME}) ---")

        # Test Case 1: Query that should find something from the dummy data (if ingested)
        test_query_1 = "What is a sample chunk?"
        response_1 = answer_query(test_query_1)  # General query

        print(f"\nQuery 1: {response_1['query']}")
        print(f"Answer 1: {response_1['answer']}")
        print(f"Chunks found: {response_1['retrieved_chunks_found']}")
        if response_1['retrieved_chunks_found']:
            print("Source Chunks (first one):")
            print(f"  Text: {response_1['source_chunks'][0]['text'][:100]}...")
            print(f"  Metadata: {response_1['source_chunks'][0]['metadata']}")
            print(f"  Distance: {response_1['source_chunks'][0]['distance']}")

        # Test Case 2: Query that likely won't find relevant chunks in dummy data
        test_query_2 = "Tell me about quantum physics."
        response_2 = answer_query(test_query_2)
        print(f"\nQuery 2: {response_2['query']}")
        print(f"Answer 2: {response_2['answer']}")
        print(f"Chunks found: {response_2['retrieved_chunks_found']}")

        # Test Case 3: Query specifically for a document that doesn't exist (using filter)
        test_query_3 = "Any information here?"
        response_3 = answer_query(test_query_3, doc_id_filter="non_existent_doc_id_123")
        print(f"\nQuery 3: {response_3['query']} (filtered for non_existent_doc_id_123)")
        print(f"Answer 3: {response_3['answer']}")
        print(f"Chunks found: {response_3['retrieved_chunks_found']}")