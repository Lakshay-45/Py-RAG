# RAG Specialist System

This project implements a Retrieval-Augmented Generation (RAG) system that allows users to upload PDF documents, ask questions based on their content, and receive contextual answers generated by a Large Language Model (LLM).

The system leverages a vector database (ChromaDB) for efficient semantic retrieval of document chunks and integrates with an LLM API (Groq, configurable for OpenAI) for response generation. The entire application is containerized using Docker for ease of deployment and consistency across environments.

**Key Features:**

*   **PDF Document Ingestion:** Upload up to 20 PDF documents at a time (max 1000 pages per document).
*   **Text Chunking & Embedding:** Documents are processed into manageable text chunks, and embeddings are generated locally using Sentence Transformers.
*   **Vector Storage:** Document chunks and their embeddings are stored in a persistent ChromaDB vector database.
*   **Metadata Management:** Document metadata (filename, status, page/chunk counts, etc.) is stored in a persistent SQLite database.
*   **Retrieval-Augmented Generation:** User queries are used to retrieve relevant document chunks, which then provide context to an LLM for generating answers.
*   **LLM Integration:** Utilizes the Groq API (featuring models like Llama 3) for fast and efficient LLM responses. Configurable to use OpenAI if preferred.
*   **RESTful API:** A FastAPI-based API provides endpoints for document management and querying.
*   **Containerized Deployment:** Dockerized for easy setup and deployment on local machines or cloud environments.

## Prerequisites

Before you begin, ensure you have the following installed:

*   [Docker](https://www.docker.com/get-started)
*   [Docker Compose](https://docs.docker.com/compose/install/) (Usually comes with Docker Desktop)
*   [Git](https://git-scm.com/downloads) (for cloning the repository)

## Setup and Installation

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/Lakshay-45/Py-RAG
    cd Py-RAG
    ```
2.  **Configure Environment Variables:**
    Create a `.env` file in the project root directory by copying the example:
    ```bash
    cp .env.example .env
    ```
    Open the `.env` file and add your API keys:

    ```env
    # .env
    # Required for LLM generation in the RAG pipeline (using Groq by default)
    GROQ_API_KEY="your_actual_groq_api_key_here"

    # Optional: If you intend to switch to OpenAI for LLM generation
    # OPENAI_API_KEY="your_actual_openai_api_key_here"
    ```
    *   You can obtain a Groq API key from [Groq Console](https://console.groq.com/keys).

3.  **Build and Run with Docker Compose:**
    From the project root directory, run the following commands:

    *   To build the Docker image:
        ```bash
        docker-compose build
        ```
    *   To start the application in detached mode (runs in the background):
        ```bash
        docker-compose up -d
        ```

4.  **Accessing the Application:**
    *   The API will be available at `http://localhost:8000`.
    *   Interactive API documentation (Swagger UI) can be accessed at `http://localhost:8000/docs`.

5.  **Viewing Logs:**
    To view the application logs from the running container:
    ```bash
    docker-compose logs -f rag_app
    ```
    Press `CTRL+C` to stop following the logs.

6.  **Stopping the Application:**
    To stop the application and remove the containers:
    ```bash
    docker-compose down
    ```
    To stop and also remove persistent volumes (this will delete your ChromaDB and SQLite data):
    ```bash
    docker-compose down -v
    ```

## API Usage

The API provides the following endpoints, accessible under the `/api/v1` prefix. You can test these using tools like `curl` or the interactive documentation at `http://localhost:8000/docs`.

### 1. Upload Documents

*   **Endpoint:** `POST /api/v1/documents/upload`
*   **Description:** Uploads one or more PDF documents for processing and ingestion.
    *   Supports up to 20 documents per request.
    *   Each document is processed up to a maximum of 1000 pages.
*   **Request:** `multipart/form-data`
    *   **Parameter:** `files` (array of files)
*   **Example `curl`:**
    ```bash
    curl -X POST \
      -F "files=@/path/to/your/document1.pdf" \
      -F "files=@/path/to/your/document2.pdf" \
      http://localhost:8000/api/v1/documents/upload
    ```
*   **Successful Response (201 Created):**
    ```json
    {
      "message": "Document processing complete. Check status for each file.",
      "processed_files": [
        {
          "filename": "document1.pdf",
          "doc_id": "unique-doc-id-1",
          "status": "completed",
          "uploaded_at": "YYYY-MM-DDTHH:MM:SS.ffffffZ",
          "processed_at": "YYYY-MM-DDTHH:MM:SS.ffffffZ",
          "num_pages": 10,
          "num_chunks": 50,
          "error_message": null
        }
      ],
      "errors": []
    }
    ```

### 2. Query System

*   **Endpoint:** `POST /api/v1/query`
*   **Description:** Submits a query to the RAG system to get an answer based on the content of ingested documents.
*   **Request Body (JSON):**
    ```json
    {
      "query": "Your question about the document content?",
      "doc_id": "optional-doc-id-to-scope-search", // Optional
      "top_k": 3 // Optional, number of chunks to retrieve (default: 3)
    }
    ```
*   **Example `curl`:**
    ```bash
    curl -X POST -H "Content-Type: application/json" \
      -d '{"query": "What are the main findings?", "top_k": 3}' \
      http://localhost:8000/api/v1/query
    ```
*   **Successful Response (200 OK):**
    ```json
    {
      "query": "What are the main findings?",
      "answer": "The main findings indicate...",
      "retrieved_chunks_found": true,
      "source_chunks": [
        {
          "text": "Relevant text chunk from a document...",
          "metadata": {
            "doc_id": "unique-doc-id-1",
            "chunk_index": 5
          },
          "distance": 0.345
        }
      ]
    }
    ```

### 3. List All Documents Metadata

*   **Endpoint:** `GET /api/v1/documents`
*   **Description:** Retrieves a paginated list of metadata for all processed documents.
*   **Query Parameters:**
    *   `skip` (integer, default: 0): Number of records to skip for pagination.
    *   `limit` (integer, default: 10, max: 100): Maximum number of records to return.
*   **Example `curl`:**
    ```bash
    curl "http://localhost:8000/api/v1/documents?skip=0&limit=5"
    ```
*   **Successful Response (200 OK):**
    ```json
    [
      {
        "filename": "document1.pdf",
        "doc_id": "unique-doc-id-1",
        "status": "completed",
        // ... other metadata fields
      },
      {
        "filename": "document2.pdf",
        "doc_id": "unique-doc-id-2",
        "status": "failed",
        "error_message": "Could not extract text.",
        // ... other metadata fields
      }
    ]
    ```

### 4. Get Specific Document Metadata

*   **Endpoint:** `GET /api/v1/documents/{doc_id}`
*   **Description:** Retrieves metadata for a single document specified by its `doc_id`.
*   **Path Parameter:** `doc_id` (string, UUID format)
*   **Example `curl`:**
    ```bash
    curl http://localhost:8000/api/v1/documents/your_document_id_here
    ```
*   **Successful Response (200 OK):**
    ```json
    {
      "filename": "document1.pdf",
      "doc_id": "your_document_id_here",
      "status": "completed",
      // ... other metadata fields
    }
    ```
*   **Error Response (404 Not Found):** If `doc_id` does not exist.

## Configuration Details

### LLM Provider

*   The system is configured to use the **Groq API** by default for LLM-based response generation.
    *   Requires a `GROQ_API_KEY` to be set in the `.env` file.
    *   The default model used is `llama3-8b-8192` (specified in `app/services/rag_pipeline.py`).
*   **Switching to OpenAI (Optional):**
    1.  Ensure your `OPENAI_API_KEY` is set in the `.env` file.
    2.  Modify the `LLM_PROVIDER` variable in `app/services/rag_pipeline.py` from `"groq"` to `"openai"`.
    3.  Rebuild the Docker image if you make code changes: `docker-compose build`.
    4.  Restart the application: `docker-compose up -d`.

### Data Persistence

*   **Vector Database (ChromaDB):** Data is stored in the `./app_data/chroma_db_data/` directory on the host machine, mapped into the container.
*   **Metadata Database (SQLite):** Data is stored in `./app_data/metadata/metadata.db` on the host machine, mapped into the container.
*   These paths are configured via Docker Compose volumes and are relative to the project root.

## Running Automated Tests

The project includes unit and integration tests written using `pytest`.

1.  **Ensure Development Dependencies are Installed:**
    If running tests outside of a pre-built Docker image (e.g., in a local virtual environment for development), make sure all dependencies from `requirements.txt` are installed, including `pytest`, `httpx`, and `reportlab`.
    ```bash
    pip install -r requirements.txt
    ```
2.  **Run Tests:**
    From the project root directory, execute:
    ```bash
    pytest -v
    ```
    This command will discover and run all tests in the `tests/` directory. The `-v` flag provides verbose output.

## Project Structure

```text
.
├── app/                  # Main application code
│   ├── __init__.py
│   ├── main.py             # FastAPI application setup
│   ├── api/              # API endpoint definitions
│   │   ├── __init__.py
│   │   └── endpoints.py
│   ├── core/             # Core configuration, settings
│   │   ├── __init__.py
│   │   └── config.py
│   ├── db/               # Database models, CRUD operations, session management
│   │   ├── __init__.py
│   │   ├── crud.py
│   │   ├── database.py
│   │   └── models.py
│   ├── models/           # Pydantic schemas for API requests/responses
│   │   ├── __init__.py
│   │   └── schemas.py
│   └── services/         # Business logic
│       ├── __init__.py
│       ├── document_processor.py
│       ├── rag_pipeline.py
│       └── vector_store_manager.py
├── app_data/             # (Created on host after first run) Persistent data (gitignored)
│   ├── chroma_db_data/   # Vector database storage
│   └── metadata/         # SQLite database storage (e.g., metadata.db)
├── data/                 # Temporary data, e.g., uploads (gitignored content)
│   └── uploads/
├── tests/                # Automated tests
│   ├── __init__.py
│   ├── integration/
│   │   ├── __init__.py
│   │   └── test_api_endpoints.py
│   └── unit/
│       ├── __init__.py
│       ├── test_crud.py
│       ├── test_document_processor.py
│       └── test_rag_pipeline.py
├── .env.example          # Example environment file
├── .gitignore            # Specifies intentionally untracked files that Git should ignore
├── Dockerfile            # Instructions to build the Docker image
├── docker-compose.yml    # Docker Compose configuration for running the service
├── entrypoint.sh         # Entrypoint script for the Docker container
├── README.md             # This file
└── requirements.txt      # Python dependencies
```

## Potential Enhancements

*   Asynchronous processing for document uploads using FastAPI's `BackgroundTasks` or a task queue like Celery for improved API responsiveness with large files.
*   More sophisticated error handling and logging.
*   Support for additional document types (e.g., .txt, .docx).
*   User authentication and authorization for API endpoints.
*   Configuration of `LLM_PROVIDER` and `LLM_MODEL_NAME` via environment variables instead of code modification.